{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0831bb3-9294-4b7d-a835-848953b9c4f8",
   "metadata": {},
   "source": [
    "### Project Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7ab4c6-9751-4faf-ad65-12e8b1b8e91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seaborn\n",
    "!pip install scipy\n",
    "!pip install scikit-learn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from scipy.stats import pointbiserialr\n",
    "from scipy.stats import chi2_contingency\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler, RobustScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac04b3e-0c91-4a2b-b04f-ffdddbad4580",
   "metadata": {},
   "source": [
    "### Citirea setului de date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c9670a-37e5-4e54-ad2c-c3720419467d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Citirea setului de date\n",
    "df = pd.read_csv('dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303a99f6-b81f-4c94-ac18-b7fff88cc5b1",
   "metadata": {},
   "source": [
    "### Filtrarea datelor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3e8bcb-40b4-429b-bfc7-4b4c671b1390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminam din dataset NaN si +- Inifity\n",
    "df = df.replace([np.inf, -np.inf], np.nan)\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e343c0-5aeb-4769-a143-b54d1b9e0b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inlocuiesc NaN si +- Infinity cu mediana pe coloane pentru\n",
    "#df = df.replace([np.inf, -np.inf], np.nan)\n",
    "#df = df.apply(lambda x: x.fillna(x.median()), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72289c18-a63e-4ddf-bb09-3a5ab314ae66",
   "metadata": {},
   "source": [
    "# 3.1. Explorarea Datelor (Exploratory Data Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd147483-f34d-4af4-bb90-d234eb113d5c",
   "metadata": {},
   "source": [
    "## 1. Analiza Echilibrului de Clase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22718d3-4280-437a-9c57-855c2c4f2eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificarea echilibrului claselor\n",
    "sns.countplot(x='Revenue', data=df)\n",
    "plt.title('Distribuția claselor pentru Revenue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a88e33-68d6-4ace-a80f-1f3578988aab",
   "metadata": {},
   "source": [
    "## 2. Vizualizarea Atributelor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41822ed-4e7f-42d3-983d-ec7c0b051e19",
   "metadata": {},
   "source": [
    "### 2.A Atribute Numerice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392cf0cd-5d7e-4202-8db4-72ec6f8c7980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificarea atributelor numerice\n",
    "numeric_attributes = df.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "# Vizualizare distribuție pentru atribute numerice\n",
    "for attribute in numeric_attributes:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.histplot(df[attribute], bins=30, kde=True)\n",
    "    plt.title(f'Distribuția atributului {attribute}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1d072f-38a0-4277-89c1-eee5530ca4c2",
   "metadata": {},
   "source": [
    "### 2.A Atribute Categorice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cac654-ecbf-4a9c-aa0b-57f5a002cb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificarea atributelor categorice\n",
    "categorical_attributes = df.select_dtypes(exclude=np.number).columns.tolist()\n",
    "\n",
    "# Vizualizare distribuție pentru atribute categorice\n",
    "for attribute in categorical_attributes:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.countplot(x=attribute, data=df, hue='Revenue')\n",
    "    plt.title(f'Distribuția atributului {attribute} în funcție de Revenue')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55bc6b1-0ee8-4794-ac30-dea77f4f417e",
   "metadata": {},
   "source": [
    "## 3. Analiza Gradului de Corelare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110726fc-a162-4148-b1b8-ba30ed2d26ce",
   "metadata": {},
   "source": [
    "### 3.A Atribute Numerice cu Coeficientul de Point-Biserial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b0fb33-a6bd-471c-8e2b-d1d8ae63c6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_results_numeric = []\n",
    "\n",
    "for attribute in numeric_attributes:\n",
    "    if attribute != 'Revenue':\n",
    "        correlation, p_value = pointbiserialr(df[attribute], df['Revenue'])\n",
    "        correlation_results_numeric.append({'Attribute': attribute, 'Correlation': correlation, 'P-Value': p_value})\n",
    "\n",
    "# Tabelul cu rezultate\n",
    "correlation_df_numeric = pd.DataFrame(correlation_results_numeric)\n",
    "print(correlation_df_numeric)\n",
    "print('\\n')\n",
    "\n",
    "# Vizualizare pentru atributele cu p-value <= 0.05\n",
    "significant_numeric_attributes = correlation_df_numeric[correlation_df_numeric['P-Value'] <= 0.05]\n",
    "plt.figure(figsize=(12, 6))\n",
    "barplot = sns.barplot(x='Attribute', y='Correlation', data=significant_numeric_attributes)\n",
    "plt.title('Coeficient de Point-Biserial pentru Atributele Numerice')\n",
    "\n",
    "# Rotirea etichetelor pe axa X pentru a face vizualizarea mai lizibilă\n",
    "X_fields = significant_numeric_attributes['Attribute'].tolist()\n",
    "barplot.set_xticks(range(len(X_fields)))\n",
    "barplot.set_xticklabels(X_fields, rotation=45, horizontalalignment='right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f407e760-7d69-46fa-8f77-113f35ac3b51",
   "metadata": {},
   "source": [
    "### 3.B Atribute Categorice cu Testul Chi-squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bc819a-dd5a-4b06-beff-27250857b643",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_results_categorical = []\n",
    "\n",
    "for attribute in categorical_attributes:\n",
    "    contingency_table = pd.crosstab(df[attribute], df['Revenue'])\n",
    "    chi2, p_value, _, _ = chi2_contingency(contingency_table)\n",
    "    correlation_results_categorical.append({'Attribute': attribute, 'Chi-squared': chi2, 'P-Value': p_value})\n",
    "\n",
    "# Tabelul cu rezultate\n",
    "correlation_df_categorical = pd.DataFrame(correlation_results_categorical)\n",
    "print(correlation_df_categorical)\n",
    "print('\\n')\n",
    "\n",
    "# Vizualizare pentru atributele cu p-value <= 0.05\n",
    "significant_categorical_attributes = correlation_df_categorical[correlation_df_categorical['P-Value'] <= 0.05]\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='Attribute', y='Chi-squared', data=significant_categorical_attributes)\n",
    "plt.title('Statistica Chi-squared pentru Atributele Categorice')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089dfbc1-823f-40e5-827a-b9ff7f40b5d4",
   "metadata": {},
   "source": [
    "# 3.2. Antrenarea și Evaluarea Algoritmilor de Predicție"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12360a73-76ff-4f72-a691-fb9e5d685ab7",
   "metadata": {},
   "source": [
    "## 3.2.1. Regresie Logistică"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f54fc8b-8053-448c-8644-43ce2542e849",
   "metadata": {},
   "source": [
    "### 1. Implementare manuală"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9878dadd-3e2b-4f85-a024-2ba6318b5fea",
   "metadata": {},
   "source": [
    "#### Regresia Logistica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de53e17-5b11-4c9f-b52b-45c9cfd89532",
   "metadata": {},
   "outputs": [],
   "source": [
    "clipped_value = 500\n",
    "\n",
    "def split_dataset(X, T, train=.8):\n",
    "    N = X.shape[0]\n",
    "    N_train = int(round(N * train))\n",
    "    N_test = N - N_train\n",
    "\n",
    "    X_train, X_test = X[:N_train, :], X[N_train:, :]\n",
    "    T_train, T_test = T[:N_train], T[N_train:]\n",
    "    return X_train, T_train, X_test, T_test\n",
    "\n",
    "def logistic(x):\n",
    "    # Clip the input values to prevent overflow\n",
    "    clipped_x = np.clip(x, -clipped_value, clipped_value)\n",
    "    \n",
    "    # Calculate the logistic function on the clipped values\n",
    "    return 1 / (1 + np.exp(-clipped_x))\n",
    "\n",
    "# Negative Log Likelihood - functia J(w)\n",
    "def nll(Y, T):\n",
    "    epsilon = 1e-15\n",
    "    Y = np.clip(Y, epsilon, 1 - epsilon)\n",
    "    return -np.mean(T * np.log(Y) + (1 - T) * np.log(1 - Y))\n",
    "\n",
    "def accuracy(Y, T):\n",
    "    predicted_labels = np.round(Y)\n",
    "    correct_predictions = np.sum(predicted_labels == T)\n",
    "    total_samples = len(T)\n",
    "    return correct_predictions / total_samples\n",
    "\n",
    "# Antrenati modelul logistic (ponderile W), executand epochs_no pasi din algoritmul de gradient descent\n",
    "def train_logistic(X, T, lr=0.01, epochs_no=100):\n",
    "    (N, D) = X.shape\n",
    "    X_hat = np.concatenate([X, np.ones((N, 1))], axis=1)\n",
    "    W = np.random.randn(D + 1)\n",
    "\n",
    "    for epoch in range(epochs_no):\n",
    "        Y = logistic(np.dot(X_hat, W))\n",
    "        gradient = np.dot(X_hat.T, (Y - T))\n",
    "        W -= lr * gradient\n",
    "\n",
    "    return W\n",
    "\n",
    "# Calculati predictia Y a modelului logistic antrenat (ponderile W invatate)\n",
    "def predict_logistic(X, W):\n",
    "    X_hat = np.concatenate([X, np.ones((X.shape[0], 1))], axis=1)\n",
    "    Y = logistic(np.dot(X_hat, W))\n",
    "    return Y\n",
    "\n",
    "def train_logistic_full(X, T, lr=0.01, learning_increase_percent=2.5, epochs_no=1000):\n",
    "    (N, D) = X.shape\n",
    "    X1 = np.concatenate([np.ones((N, 1)), X], axis=1)\n",
    "    W = np.random.randn(D + 1)\n",
    "\n",
    "    X_train, T_train, X_test, T_test = split_dataset(X1, T)\n",
    "    \n",
    "    train_acc, test_acc = [], []\n",
    "    train_nll, test_nll = [], []\n",
    "    W_trace = [W.copy()]\n",
    "\n",
    "    for epoch in range(epochs_no):\n",
    "        # Maresc learning rate-ul pt fiecare epoch\n",
    "        lr = lr * (1 + learning_increase_percent / 100) \n",
    "        \n",
    "        Y_train = logistic(X_train @ W)\n",
    "        \n",
    "        gradient = np.transpose(X_train) @ (Y_train-T_train)/N\n",
    "        W -= lr * gradient\n",
    "\n",
    "        logits = np.dot(X_test, W)\n",
    "        Y_test = 1. / (1. + np.exp(-np.clip(logits, -clipped_value, clipped_value)))\n",
    "\n",
    "        train_acc.append(accuracy(Y_train, T_train))\n",
    "        test_acc.append(accuracy(Y_test, T_test))\n",
    "        train_nll.append(nll(Y_train, T_train))\n",
    "        test_nll.append(nll(Y_test, T_test))\n",
    "        W_trace.append(W.copy())\n",
    "\n",
    "    return W, train_acc, test_acc, train_nll, test_nll, W_trace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f93b8d-b934-45e4-9f11-7cae2d9439fa",
   "metadata": {},
   "source": [
    "#### Preprocesarea datelor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a64666-aab9-4bcb-8639-7bc98e289199",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    # Split set de date în caracteristici (X) și variabilă țintă (y)\n",
    "    X = df.drop('Revenue', axis=1)\n",
    "    y = df['Revenue'] # Revenue este boolean ( 0 / 1)\n",
    "    \n",
    "    # Conversia variabilelor categorice în formă numerică\n",
    "    label_encoder = LabelEncoder()\n",
    "    categorical_columns = ['Month', 'VisitorType', 'Weekend']\n",
    "    for column in categorical_columns:\n",
    "        X_train[column] = label_encoder.fit_transform(X_train[column])\n",
    "        X_test[column] = label_encoder.transform(X_test[column])  # Utilizăm aceeași transformare pe setul de test\n",
    "    \n",
    "    # Normalizarea datelor (Scalare)\n",
    "    scalers = {\n",
    "        'MinMaxScaler': MinMaxScaler(),\n",
    "        'StandardScaler': StandardScaler(),\n",
    "        'RobustScaler': RobustScaler()\n",
    "    }\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc47e0a-7947-431a-a17a-ab50f12faae8",
   "metadata": {},
   "source": [
    "#### Antrenare si Testare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e146ea7d-1d3e-44af-a723-508c2e481895",
   "metadata": {},
   "source": [
    "### Functie pentru plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1468658a-56a9-47b4-8792-1cd4473a61e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definim o functie ajutatoare pentru plotting\n",
    "def plot_evolution(train_acc, test_acc, train_nll, test_nll, scaler, step_percent=2.5):\n",
    "    # Step_percent = 5 <==> 100 / 5 == 20 X points on the plot\n",
    "    epochs_no = len(train_acc)\n",
    "    step = round(epochs_no * step_percent / 100)\n",
    "    # If step is 0, we will set minimum default step for the plotting\n",
    "    if step == 0:\n",
    "        step = 1\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1)\n",
    "\n",
    "    # Plotting accuracy evolution\n",
    "    ax1.plot(range(0, epochs_no, step), train_acc[::step], sns.xkcd_rgb[\"green\"], label=\"Train Accuracy\")\n",
    "    ax1.plot(range(0, epochs_no, step), test_acc[::step], sns.xkcd_rgb[\"red\"], label=\"Test Accuracy\")\n",
    "    ax1.set_ylabel(\"Accuracy\")\n",
    "    ax1.legend(loc='lower right', ncol=1)\n",
    "\n",
    "    # Plotting negative log likelihood evolution\n",
    "    ax2.plot(range(0, epochs_no, step), train_nll[::step], sns.xkcd_rgb[\"green\"], label=\"Train NLL\")\n",
    "    ax2.plot(range(0, epochs_no, step), test_nll[::step], sns.xkcd_rgb[\"red\"], label=\"Test NLL\")\n",
    "    ax2.set_xlabel(\"Epoch\")\n",
    "    ax2.set_ylabel(\"NLL\")\n",
    "    ax2.legend(loc='upper right', ncol=1)\n",
    "\n",
    "    # Adding title with scaler information\n",
    "    plt.suptitle(f'Evolution with Scaler: {scaler}')\n",
    "    plt.show()\n",
    "    \n",
    "    return (ax1, ax2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df4b138-d756-4c34-8096-96240ac53572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to print metrics for a specific measure (e.g., accuracy, precision, recall, f1_score)\n",
    "def print_metrics(metric_name, results_dict, num_iterations):\n",
    "    for scaler_name, metric_list in results_dict.items():\n",
    "        mean_metric = np.mean(metric_list)\n",
    "        median_metric = np.median(metric_list)\n",
    "        variance_metric = np.var(metric_list)\n",
    "        min_metric = np.min(metric_list)\n",
    "        max_metric = np.max(metric_list)\n",
    "\n",
    "        print(f\"\\nMetrics for {metric_name} with {scaler_name} after {num_iterations} runs:\")\n",
    "        print(f\"Mean {metric_name}: {mean_metric}\")\n",
    "        print(f\"Median {metric_name}: {median_metric}\")\n",
    "        print(f\"Variance {metric_name}: {variance_metric}\")\n",
    "        print(f\"Min {metric_name}: {min_metric}\")\n",
    "        print(f\"Max {metric_name}: {max_metric}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4065cf-96fe-4ed0-8e54-83d81c16206e",
   "metadata": {},
   "source": [
    "#### Datele de fitting ale modelului"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc43fcca-1ee8-446b-a0c7-8444c2e74228",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS_NO = 200\n",
    "LR = 0.035\n",
    "LEARNING_INCREASE_PERCENT = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c7e5ca-ec8a-4c2d-a433-3b96dc00c645",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_manual(X, y, lr=0.01, learning_increase_percent=2.5, epochs_no=100, step_percent=100):   \n",
    "    for scaler_name, scaler in scalers.items():\n",
    "        # Scalarea datelor de antrenare\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "        # Antrenarea modelului logistic\n",
    "        W, train_acc, test_acc, train_nll, test_nll, W_trace = train_logistic_full(X_train_scaled, y_train, lr=lr, epochs_no=epochs_no)\n",
    "\n",
    "        # Evaluarea modelului pe setul de testare\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        Y_test = predict_logistic(X_test_scaled, W)\n",
    "\n",
    "        # Evaluare metrici pe setul de testare\n",
    "        precision = precision_score(y_test, np.round(Y_test))\n",
    "        recall = recall_score(y_test, np.round(Y_test))\n",
    "        f1 = f1_score(y_test, np.round(Y_test))\n",
    "        test_accuracy = accuracy(Y_test, y_test)\n",
    "\n",
    "        # Afișare și salvare metrici\n",
    "        print(f\"Acuratete folosind {scaler_name}: {test_accuracy}\")\n",
    "        print(f\"Precision folosind {scaler_name}: {precision}\")\n",
    "        print(f\"Recall folosind {scaler_name}: {recall}\")\n",
    "        print(f\"F1 Score folosind {scaler_name}: {f1}\")\n",
    "        \n",
    "        # Save metric values in dictionaries\n",
    "        if scaler_name not in precision_results:\n",
    "            precision_results[scaler_name] = []\n",
    "        precision_results[scaler_name].append(precision)\n",
    "        if scaler_name not in recall_results:\n",
    "            recall_results[scaler_name] = []\n",
    "        recall_results[scaler_name].append(recall)\n",
    "        if scaler_name not in f1_score_results:\n",
    "            f1_score_results[scaler_name] = []\n",
    "        f1_score_results[scaler_name].append(f1)\n",
    "        if scaler_name not in accuracy_results:\n",
    "            accuracy_results[scaler_name] = []\n",
    "        accuracy_results[scaler_name].append(test_accuracy)\n",
    "\n",
    "        # Vizualizare evolutie\n",
    "        plot_evolution(train_acc, test_acc, train_nll, test_nll, scaler, step_percent)\n",
    "        \n",
    "        # Eliberare memorie\n",
    "        del X_train_scaled, X_test_scaled, Y_test, W, train_acc, test_acc, train_nll, test_nll, W_trace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641771e3-753b-4db6-95da-d9b6f0362de9",
   "metadata": {},
   "source": [
    "### Rulam de 10 ori algoritmul pe sample-uri random ale datasetului initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32523e3e-c02a-4345-a21e-94285306052e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot X axis step\n",
    "STEP_PERCENT=1.25\n",
    "# Number of times to run the function\n",
    "num_iterations = 10\n",
    "\n",
    "# Print information about the training dataset\n",
    "(N, D) = df.shape\n",
    "print(f\"Training dataset size: {N}\")\n",
    "print(f\"Number of features: {D - 1}\")\n",
    "\n",
    "# Create dictionaries to store metric values\n",
    "precision_results = {}\n",
    "recall_results = {}\n",
    "f1_score_results = {}\n",
    "accuracy_results = {}\n",
    "\n",
    "for iteration in range(num_iterations):\n",
    "    # Amestecarea (randomizarea) rândurilor\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    X, y = preprocess_data(df.copy())\n",
    "\n",
    "    print(f\"\\nRULAREA NUMARUL {iteration + 1}\")\n",
    "    # Apelul funcției cu datele specifice\n",
    "    logistic_regression_manual(X,y,lr=LR,learning_increase_percent=LEARNING_INCREASE_PERCENT,epochs_no=EPOCHS_NO,step_percent=STEP_PERCENT)\n",
    "    del X, y\n",
    "\n",
    "print(\"\\nFINAL STATISTICS\")\n",
    "# Print accuracy metrics\n",
    "print(\"\\nACCURACY\")\n",
    "print_metrics(\"Accuracy\", accuracy_results, num_iterations)\n",
    "# Print precision metrics\n",
    "print(\"\\nPRECISION\")\n",
    "print_metrics(\"Precision\", precision_results, num_iterations)\n",
    "# Print recall metrics\n",
    "print(\"\\nRECALL\")\n",
    "print_metrics(\"Recall\", recall_results, num_iterations)\n",
    "# Print F1 score metrics\n",
    "print(\"\\nF1\")\n",
    "print_metrics(\"F1 Score\", f1_score_results, num_iterations)\n",
    "\n",
    "del precision_results, recall_results, f1_score_results, accuracy_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3f8a05-9f60-4b4c-9982-27710c342b44",
   "metadata": {},
   "source": [
    "### 2. Implementare folosind scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa17261a-87fa-4967-9e3d-2bd8218c4536",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for scaler_name, scaler in scalers.items():\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Antrenarea modelului\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluarea modelului\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    results.append({\n",
    "        'Scaler': scaler_name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1\n",
    "    })\n",
    "\n",
    "# Afisarea rezultatelor\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63411ad7-2a52-4049-a44a-9a4db117fdd6",
   "metadata": {},
   "source": [
    "## 3.2.2. Arbore de Decizie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7f57b5-ddc8-448e-b6bc-b0057e174a56",
   "metadata": {},
   "source": [
    "### 1. Implementare folosind scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993dca31-93c1-4ba0-a021-5ec0b8f1e4ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "97881e82-f007-4a23-bdde-2764465a8810",
   "metadata": {},
   "source": [
    "### 2. Implementare manuală"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92b40e8-fd6c-4e30-b719-d7dc7b1fb8b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
