{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0831bb3-9294-4b7d-a835-848953b9c4f8",
   "metadata": {},
   "source": [
    "### Project Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7ab4c6-9751-4faf-ad65-12e8b1b8e91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seaborn\n",
    "!pip install scipy\n",
    "!pip install scikit-learn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from scipy.stats import pointbiserialr\n",
    "from scipy.stats import chi2_contingency\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler, RobustScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac04b3e-0c91-4a2b-b04f-ffdddbad4580",
   "metadata": {},
   "source": [
    "### Citirea setului de date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c9670a-37e5-4e54-ad2c-c3720419467d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Citirea setului de date\n",
    "df = pd.read_csv('dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303a99f6-b81f-4c94-ac18-b7fff88cc5b1",
   "metadata": {},
   "source": [
    "### Filtrarea datelor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3e8bcb-40b4-429b-bfc7-4b4c671b1390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminam din dataset NaN si +- Inifity\n",
    "df = df.replace([np.inf, -np.inf], np.nan)\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e343c0-5aeb-4769-a143-b54d1b9e0b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inlocuiesc NaN si +- Infinity cu mediana pe coloane pentru\n",
    "#df = df.replace([np.inf, -np.inf], np.nan)\n",
    "#df = df.apply(lambda x: x.fillna(x.median()), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72289c18-a63e-4ddf-bb09-3a5ab314ae66",
   "metadata": {},
   "source": [
    "# 3.1. Explorarea Datelor (Exploratory Data Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd147483-f34d-4af4-bb90-d234eb113d5c",
   "metadata": {},
   "source": [
    "## 1. Analiza Echilibrului de Clase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22718d3-4280-437a-9c57-855c2c4f2eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificarea echilibrului claselor\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='Revenue', data=df)\n",
    "plt.title('Distribuția claselor pentru Revenue')\n",
    "plt.xlabel('Revenue')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a88e33-68d6-4ace-a80f-1f3578988aab",
   "metadata": {},
   "source": [
    "## 2. Vizualizarea Atributelor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41822ed-4e7f-42d3-983d-ec7c0b051e19",
   "metadata": {},
   "source": [
    "### 2.A Atribute Numerice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392cf0cd-5d7e-4202-8db4-72ec6f8c7980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificarea atributelor numerice\n",
    "numeric_features = df.select_dtypes(include=[np.number]).columns\n",
    "for feature in numeric_features:\n",
    "    # Calculați percentilele cu granularitate de 10%\n",
    "    percentiles = np.percentile(df[feature], np.arange(0, 101, 10))\n",
    "    \n",
    "    # Alegem pragul pentru a elimina outlierii (de exemplu, 90th percentile)\n",
    "    threshold = percentiles[9]\n",
    "    \n",
    "    # Filtrați datele pentru a elimina outlierii\n",
    "    filtered_data = df[df[feature] <= threshold]\n",
    "    \n",
    "    # Realizați un grafic pentru distribuția valorilor fără outlieri\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.histplot(filtered_data[feature], bins=50, kde=True)\n",
    "    \n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Count')\n",
    "    plt.title(f'Distribution of {feature} without Outliers (<= {int(threshold)})')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1d072f-38a0-4277-89c1-eee5530ca4c2",
   "metadata": {},
   "source": [
    "### 2.A Atribute Categorice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cac654-ecbf-4a9c-aa0b-57f5a002cb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificarea atributelor categorice\n",
    "categorical_attributes = df.select_dtypes(exclude=np.number).columns.tolist()\n",
    "\n",
    "for attribute in categorical_attributes:\n",
    "    # Calculăm frecvența pentru fiecare categorie\n",
    "    category_counts = df[attribute].value_counts()\n",
    "\n",
    "    # Alegem un prag pentru a elimina categoriile cu o frecvență mai mare decât percentilele specificate (de exemplu, 90th percentile)\n",
    "    threshold = np.percentile(category_counts, 90)\n",
    "\n",
    "    # Filtrăm datele pentru a păstra doar categoriile cu frecvență sub prag\n",
    "    df_filtered = df[df[attribute].isin(category_counts[category_counts <= threshold].index)]\n",
    "\n",
    "    # Realizăm un grafic pentru distribuția categoriilor după eliminarea outlierilor\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.histplot(x=attribute, data=df_filtered, hue='Revenue', multiple='stack', shrink=0.8)\n",
    "    plt.title(f'Distribuția atributului {attribute} în funcție de Revenue (fără outlieri)')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55bc6b1-0ee8-4794-ac30-dea77f4f417e",
   "metadata": {},
   "source": [
    "## 3. Analiza Gradului de Corelare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110726fc-a162-4148-b1b8-ba30ed2d26ce",
   "metadata": {},
   "source": [
    "### 3.A Atribute Numerice cu Coeficientul de Point-Biserial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b0fb33-a6bd-471c-8e2b-d1d8ae63c6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_results_numeric = []\n",
    "\n",
    "for attribute in numeric_attributes:\n",
    "    if attribute != 'Revenue':\n",
    "        correlation, p_value = pointbiserialr(df[attribute], df['Revenue'])\n",
    "        correlation_results_numeric.append({'Attribute': attribute, 'Correlation': correlation, 'P-Value': p_value})\n",
    "\n",
    "# Tabelul cu rezultate\n",
    "correlation_df_numeric = pd.DataFrame(correlation_results_numeric)\n",
    "print(correlation_df_numeric)\n",
    "print('\\n')\n",
    "\n",
    "# Vizualizare pentru atributele cu p-value <= 0.05\n",
    "significant_numeric_attributes = correlation_df_numeric[correlation_df_numeric['P-Value'] <= 0.05]\n",
    "plt.figure(figsize=(12, 6))\n",
    "barplot = sns.barplot(x='Attribute', y='Correlation', data=significant_numeric_attributes)\n",
    "plt.title('Coeficient de Point-Biserial pentru Atributele Numerice')\n",
    "\n",
    "# Rotirea etichetelor pe axa X pentru a face vizualizarea mai lizibilă\n",
    "X_fields = significant_numeric_attributes['Attribute'].tolist()\n",
    "barplot.set_xticks(range(len(X_fields)))\n",
    "barplot.set_xticklabels(X_fields, rotation=45, horizontalalignment='right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f407e760-7d69-46fa-8f77-113f35ac3b51",
   "metadata": {},
   "source": [
    "### 3.B Atribute Categorice cu Testul Chi-squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bc819a-dd5a-4b06-beff-27250857b643",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_results_categorical = []\n",
    "\n",
    "for attribute in categorical_attributes:\n",
    "    contingency_table = pd.crosstab(df[attribute], df['Revenue'])\n",
    "    chi2, p_value, _, _ = chi2_contingency(contingency_table)\n",
    "    correlation_results_categorical.append({'Attribute': attribute, 'Chi-squared': chi2, 'P-Value': p_value})\n",
    "\n",
    "# Tabelul cu rezultate\n",
    "correlation_df_categorical = pd.DataFrame(correlation_results_categorical)\n",
    "print(correlation_df_categorical)\n",
    "print('\\n')\n",
    "\n",
    "# Vizualizare pentru atributele cu p-value <= 0.05\n",
    "significant_categorical_attributes = correlation_df_categorical[correlation_df_categorical['P-Value'] <= 0.05]\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='Attribute', y='Chi-squared', data=significant_categorical_attributes)\n",
    "plt.title('Statistica Chi-squared pentru Atributele Categorice')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cea620-5c63-42e8-9a0b-39a3a27eccbd",
   "metadata": {},
   "source": [
    "### Concluzia analizei p-value folosind Coeficientul de Point-Biserial si Testul Chi-squared\n",
    "- Un p-value cat mai mic indica o corelare mai mare a atributului cu tinta de predictie\n",
    "- Vom folosi p-valiue <= 0.05 ca valoare maxima admisibila pentru un atribut, pentru a-l considera relrevant in predictia noastra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089dfbc1-823f-40e5-827a-b9ff7f40b5d4",
   "metadata": {},
   "source": [
    "# 3.2. Antrenarea și Evaluarea Algoritmilor de Predicție"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fa6f5e-c642-4800-81bd-ea7ce95ee212",
   "metadata": {},
   "source": [
    "#### Setting some variables needed for the comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551eecc3-c0ff-4cb9-8d30-22e9c189c89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot X axis step\n",
    "STEP_PERCENT=1.25\n",
    "# Number of times to run the function\n",
    "num_iterations = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12360a73-76ff-4f72-a691-fb9e5d685ab7",
   "metadata": {},
   "source": [
    "## 3.2.1. Regresie Logistică"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f54fc8b-8053-448c-8644-43ce2542e849",
   "metadata": {},
   "source": [
    "### 1. Implementare manuală"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9878dadd-3e2b-4f85-a024-2ba6318b5fea",
   "metadata": {},
   "source": [
    "#### Regresia Logistica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de53e17-5b11-4c9f-b52b-45c9cfd89532",
   "metadata": {},
   "outputs": [],
   "source": [
    "clipped_value = 500\n",
    "\n",
    "def split_dataset(X, T, train=.8):\n",
    "    N = X.shape[0]\n",
    "    N_train = int(round(N * train))\n",
    "    N_test = N - N_train\n",
    "\n",
    "    X_train, X_test = X[:N_train, :], X[N_train:, :]\n",
    "    T_train, T_test = T[:N_train], T[N_train:]\n",
    "    return X_train, T_train, X_test, T_test\n",
    "\n",
    "def logistic(x):\n",
    "    # Clip the input values to prevent overflow\n",
    "    clipped_x = np.clip(x, -clipped_value, clipped_value)\n",
    "    \n",
    "    # Calculate the logistic function on the clipped values\n",
    "    return 1 / (1 + np.exp(-clipped_x))\n",
    "\n",
    "# Negative Log Likelihood - functia J(w)\n",
    "def nll(Y, T):\n",
    "    epsilon = 1e-15\n",
    "    Y = np.clip(Y, epsilon, 1 - epsilon)\n",
    "    return -np.mean(T * np.log(Y) + (1 - T) * np.log(1 - Y))\n",
    "\n",
    "def accuracy(Y, T):\n",
    "    predicted_labels = np.round(Y)\n",
    "    correct_predictions = np.sum(predicted_labels == T)\n",
    "    total_samples = len(T)\n",
    "    return correct_predictions / total_samples\n",
    "\n",
    "# Antrenati modelul logistic (ponderile W), executand epochs_no pasi din algoritmul de gradient descent\n",
    "def train_logistic(X, T, lr=0.01, epochs_no=100):\n",
    "    (N, D) = X.shape\n",
    "    X_hat = np.concatenate([X, np.ones((N, 1))], axis=1)\n",
    "    W = np.random.randn(D + 1)\n",
    "\n",
    "    for epoch in range(epochs_no):\n",
    "        Y = logistic(np.dot(X_hat, W))\n",
    "        gradient = np.dot(X_hat.T, (Y - T))\n",
    "        W -= lr * gradient\n",
    "\n",
    "    return W\n",
    "\n",
    "# Calculati predictia Y a modelului logistic antrenat (ponderile W invatate)\n",
    "def predict_logistic(X, W):\n",
    "    X_hat = np.concatenate([X, np.ones((X.shape[0], 1))], axis=1)\n",
    "    Y = logistic(np.dot(X_hat, W))\n",
    "    return Y\n",
    "\n",
    "def train_logistic_full(X, T, lr=0.01, learning_increase_percent=2.5, epochs_no=1000):\n",
    "    (N, D) = X.shape\n",
    "    X1 = np.concatenate([np.ones((N, 1)), X], axis=1)\n",
    "    W = np.random.randn(D + 1)\n",
    "\n",
    "    X_train, T_train, X_test, T_test = split_dataset(X1, T)\n",
    "    \n",
    "    train_acc, test_acc = [], []\n",
    "    train_nll, test_nll = [], []\n",
    "    W_trace = [W.copy()]\n",
    "\n",
    "    for epoch in range(epochs_no):\n",
    "        # Maresc learning rate-ul pt fiecare epoch\n",
    "        lr = lr * (1 + learning_increase_percent / 100) \n",
    "        \n",
    "        Y_train = logistic(X_train @ W)\n",
    "        \n",
    "        gradient = np.transpose(X_train) @ (Y_train-T_train)/N\n",
    "        W -= lr * gradient\n",
    "\n",
    "        logits = np.dot(X_test, W)\n",
    "        Y_test = 1. / (1. + np.exp(-np.clip(logits, -clipped_value, clipped_value)))\n",
    "\n",
    "        train_acc.append(accuracy(Y_train, T_train))\n",
    "        test_acc.append(accuracy(Y_test, T_test))\n",
    "        train_nll.append(nll(Y_train, T_train))\n",
    "        test_nll.append(nll(Y_test, T_test))\n",
    "        W_trace.append(W.copy())\n",
    "\n",
    "    return W, train_acc, test_acc, train_nll, test_nll, W_trace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f93b8d-b934-45e4-9f11-7cae2d9439fa",
   "metadata": {},
   "source": [
    "#### Preprocesarea datelor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a64666-aab9-4bcb-8639-7bc98e289199",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    # Split set de date în caracteristici (X) și variabilă țintă (y)\n",
    "    X = df.drop('Revenue', axis=1)\n",
    "    y = df['Revenue'] # Revenue este boolean ( 0 / 1)\n",
    "    \n",
    "    # Conversia variabilelor categorice în formă numerică\n",
    "    label_encoder = LabelEncoder()\n",
    "    categorical_columns = ['Month', 'VisitorType', 'Weekend']\n",
    "    for column in categorical_columns:\n",
    "        X[column] = label_encoder.fit_transform(X[column])\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b333605f-0918-42e2-bdad-7d76a1360010",
   "metadata": {},
   "source": [
    "#### Functie pentru plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1468658a-56a9-47b4-8792-1cd4473a61e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definim o functie ajutatoare pentru plotting\n",
    "def plot_evolution(train_acc, test_acc, train_nll, test_nll, scaler, step_percent=2.5):\n",
    "    # Step_percent = 5 <==> 100 / 5 == 20 X points on the plot\n",
    "    epochs_no = len(train_acc)\n",
    "    step = round(epochs_no * step_percent / 100)\n",
    "    # If step is 0, we will set minimum default step for the plotting\n",
    "    if step == 0:\n",
    "        step = 1\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1)\n",
    "\n",
    "    # Plotting accuracy evolution\n",
    "    ax1.plot(range(0, epochs_no, step), train_acc[::step], sns.xkcd_rgb[\"green\"], label=\"Train Accuracy\")\n",
    "    ax1.plot(range(0, epochs_no, step), test_acc[::step], sns.xkcd_rgb[\"red\"], label=\"Test Accuracy\")\n",
    "    ax1.set_ylabel(\"Accuracy\")\n",
    "    ax1.legend(loc='lower right', ncol=1)\n",
    "\n",
    "    # Plotting negative log likelihood evolution\n",
    "    ax2.plot(range(0, epochs_no, step), train_nll[::step], sns.xkcd_rgb[\"green\"], label=\"Train NLL\")\n",
    "    ax2.plot(range(0, epochs_no, step), test_nll[::step], sns.xkcd_rgb[\"red\"], label=\"Test NLL\")\n",
    "    ax2.set_xlabel(\"Epoch\")\n",
    "    ax2.set_ylabel(\"NLL\")\n",
    "    ax2.legend(loc='upper right', ncol=1)\n",
    "\n",
    "    # Adding title with scaler information\n",
    "    plt.suptitle(f'Evolution with Scaler: {scaler}')\n",
    "    plt.show()\n",
    "    \n",
    "    return (ax1, ax2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df4b138-d756-4c34-8096-96240ac53572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to print metrics for a specific measure (e.g., accuracy, precision, recall, f1_score)\n",
    "def print_metrics(metric_name, results_dict, num_iterations):\n",
    "    for scaler_name, metric_list in results_dict.items():\n",
    "        mean_metric = np.mean(metric_list)\n",
    "        median_metric = np.median(metric_list)\n",
    "        variance_metric = np.var(metric_list)\n",
    "        min_metric = np.min(metric_list)\n",
    "        max_metric = np.max(metric_list)\n",
    "\n",
    "        print(f\"\\nMetrics for {metric_name} with {scaler_name} after {num_iterations} runs:\")\n",
    "        print(f\"Mean {metric_name}: {mean_metric}\")\n",
    "        print(f\"Median {metric_name}: {median_metric}\")\n",
    "        print(f\"Variance {metric_name}: {variance_metric}\")\n",
    "        print(f\"Min {metric_name}: {min_metric}\")\n",
    "        print(f\"Max {metric_name}: {max_metric}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4065cf-96fe-4ed0-8e54-83d81c16206e",
   "metadata": {},
   "source": [
    "#### Datele de fitting ale modelului"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc43fcca-1ee8-446b-a0c7-8444c2e74228",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS_NO = 200\n",
    "LR = 0.035\n",
    "LEARNING_INCREASE_PERCENT = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d89c0ed-71cf-4c29-8dba-ed8839d0c623",
   "metadata": {},
   "source": [
    "### Scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadfe13b-9c83-498b-a742-6b6bcdb300b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizarea datelor (Scalare)\n",
    "scalers = {\n",
    "        'MinMaxScaler': MinMaxScaler(),\n",
    "        'StandardScaler': StandardScaler(),\n",
    "        'RobustScaler': RobustScaler()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5220870e-898c-45a7-a960-6f3cc81dcfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_manual(X, y, lr=0.01, learning_increase_percent=2.5, epochs_no=100, step_percent=100):\n",
    "    # Modify the dataset to include only the features we consider relevant to our target\n",
    "    # Lista cu numele caracteristicilor selectate pe baza p-value-ului\n",
    "    selected_features = ['Administrative', 'Administrative_Duration', 'Informational', 'Informational_Duration',\n",
    "                     'ProductRelated', 'ProductRelated_Duration', 'BounceRates', 'ExitRates', 'PageValues',\n",
    "                     'SpecialDay', 'OperatingSystems', 'Browser', 'Region', 'TrafficType', 'Month', 'VisitorType', 'Weekend']\n",
    "    X_selected = X[selected_features]\n",
    "    \n",
    "    # Iterate over the scalers\n",
    "    for scaler_name, scaler in scalers.items():\n",
    "        # Split the dataset into training and testing sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "        \n",
    "        # Scalarea datelor de antrenare\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "        # Antrenarea modelului logistic\n",
    "        W, train_acc, test_acc, train_nll, test_nll, W_trace = train_logistic_full(X_train_scaled, y_train, lr=lr, epochs_no=epochs_no)\n",
    "\n",
    "        # Evaluarea modelului pe setul de testare\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        Y_test = predict_logistic(X_test_scaled, W)\n",
    "\n",
    "        # Evaluare metrici pe setul de testare\n",
    "        precision = precision_score(y_test, np.round(Y_test))\n",
    "        recall = recall_score(y_test, np.round(Y_test))\n",
    "        f1 = f1_score(y_test, np.round(Y_test))\n",
    "        test_accuracy = accuracy(Y_test, y_test)\n",
    "\n",
    "        # Afișare și salvare metrici\n",
    "        print(f\"Acuratete folosind {scaler_name}: {test_accuracy}\")\n",
    "        print(f\"Precision folosind {scaler_name}: {precision}\")\n",
    "        print(f\"Recall folosind {scaler_name}: {recall}\")\n",
    "        print(f\"F1 Score folosind {scaler_name}: {f1}\")\n",
    "        \n",
    "        # Save metric values in dictionaries\n",
    "        if scaler_name not in precision_results:\n",
    "            precision_results[scaler_name] = []\n",
    "        precision_results[scaler_name].append(precision)\n",
    "        if scaler_name not in recall_results:\n",
    "            recall_results[scaler_name] = []\n",
    "        recall_results[scaler_name].append(recall)\n",
    "        if scaler_name not in f1_score_results:\n",
    "            f1_score_results[scaler_name] = []\n",
    "        f1_score_results[scaler_name].append(f1)\n",
    "        if scaler_name not in accuracy_results:\n",
    "            accuracy_results[scaler_name] = []\n",
    "        accuracy_results[scaler_name].append(test_accuracy)\n",
    "\n",
    "        # Vizualizare evolutie\n",
    "        plot_evolution(train_acc, test_acc, train_nll, test_nll, scaler, step_percent)\n",
    "        \n",
    "        # Eliberare memorie\n",
    "        del X_train, X_train_scaled, X_test_scaled, Y_test, W, train_acc, test_acc, train_nll, test_nll, W_trace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a74586-c206-41a8-ab15-57d303c8d8fe",
   "metadata": {},
   "source": [
    "#### Rulam de 10 ori algoritmul pe sample-uri random ale datasetului initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32523e3e-c02a-4345-a21e-94285306052e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print information about the training dataset\n",
    "(N, D) = df.shape\n",
    "print(f\"Training dataset size: {N}\")\n",
    "print(f\"Number of features: {D - 1}\")\n",
    "\n",
    "# Create dictionaries to store metric values\n",
    "precision_results = {}\n",
    "recall_results = {}\n",
    "f1_score_results = {}\n",
    "accuracy_results = {}\n",
    "\n",
    "for iteration in range(num_iterations):\n",
    "    # Amestecarea (randomizarea) rândurilor\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    # Preprocesarea datelor\n",
    "    X, y = preprocess_data(df.copy())\n",
    "\n",
    "    print(f\"\\nRULAREA NUMARUL {iteration + 1}\")\n",
    "    # Apelul funcției cu datele specifice\n",
    "    logistic_regression_manual(X,y,lr=LR,learning_increase_percent=LEARNING_INCREASE_PERCENT,epochs_no=EPOCHS_NO,step_percent=STEP_PERCENT)\n",
    "    del X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441333a1-6e1e-4494-a8fd-21b656e3afda",
   "metadata": {},
   "source": [
    "#### Conclusion for the manual logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd86c67-97c2-4723-a807-1f8019b8583d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nFINAL STATISTICS\")\n",
    "# Print accuracy metrics\n",
    "print(\"\\nACCURACY\")\n",
    "print_metrics(\"Accuracy\", accuracy_results, num_iterations)\n",
    "# Print precision metrics\n",
    "print(\"\\nPRECISION\")\n",
    "print_metrics(\"Precision\", precision_results, num_iterations)\n",
    "# Print recall metrics\n",
    "print(\"\\nRECALL\")\n",
    "print_metrics(\"Recall\", recall_results, num_iterations)\n",
    "# Print F1 score metrics\n",
    "print(\"\\nF1\")\n",
    "print_metrics(\"F1 Score\", f1_score_results, num_iterations)\n",
    "\n",
    "del precision_results, recall_results, f1_score_results, accuracy_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3f8a05-9f60-4b4c-9982-27710c342b44",
   "metadata": {},
   "source": [
    "### 2. Implementare folosind scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05fee92-4d39-49b6-be66-c1f6112a074b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_scikit_learn(X, y, step_percent=100):\n",
    "    # Iterate over the scalers\n",
    "    for scaler_name, scaler in scalers.items():\n",
    "        # Split the dataset into training and testing sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "        \n",
    "        # Scalarea datelor de antrenare\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "    \n",
    "        # Create and train the logistic regression model\n",
    "        model = LogisticRegression(random_state=None, max_iter=5000)\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "\n",
    "        # Make predictions on the test set\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        \n",
    "        # Evaluate the model\n",
    "        test_accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "        # Afișare și salvare metrici\n",
    "        print(f\"Acuratete folosind {scaler_name}: {test_accuracy}\")\n",
    "        print(f\"Precision folosind {scaler_name}: {precision}\")\n",
    "        print(f\"Recall folosind {scaler_name}: {recall}\")\n",
    "        print(f\"F1 Score folosind {scaler_name}: {f1}\")\n",
    "        \n",
    "        # Save metric values in dictionaries\n",
    "        if scaler_name not in precision_results:\n",
    "            precision_results[scaler_name] = []\n",
    "        precision_results[scaler_name].append(precision)\n",
    "        if scaler_name not in recall_results:\n",
    "            recall_results[scaler_name] = []\n",
    "        recall_results[scaler_name].append(recall)\n",
    "        if scaler_name not in f1_score_results:\n",
    "            f1_score_results[scaler_name] = []\n",
    "        f1_score_results[scaler_name].append(f1)\n",
    "        if scaler_name not in accuracy_results:\n",
    "            accuracy_results[scaler_name] = []\n",
    "        accuracy_results[scaler_name].append(test_accuracy)\n",
    "\n",
    "        # Clean up variables\n",
    "        del model\n",
    "\n",
    "    # Clean up remaining variables\n",
    "    del X_train, X_test, y_train, y_test, X_train_scaled, X_test_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46ae654-b096-413c-8bad-afb420d0c719",
   "metadata": {},
   "source": [
    "#### Rulam de 10 ori algoritmul pe sample-uri random ale datasetului initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa17261a-87fa-4967-9e3d-2bd8218c4536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print information about the training dataset\n",
    "(N, D) = df.shape\n",
    "print(f\"Training dataset size: {N}\")\n",
    "print(f\"Number of features: {D - 1}\")\n",
    "\n",
    "# Create dictionaries to store metric values\n",
    "precision_results = {}\n",
    "recall_results = {}\n",
    "f1_score_results = {}\n",
    "accuracy_results = {}\n",
    "\n",
    "for iteration in range(num_iterations):\n",
    "    # Amestecarea (randomizarea) rândurilor\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    X, y = preprocess_data(df.copy())\n",
    "\n",
    "    print(f\"\\nRUN NUMBER {iteration + 1}\")\n",
    "    # Apelul funcției cu datele specifice\n",
    "    logistic_regression_scikit_learn(X, y, step_percent=STEP_PERCENT)\n",
    "    del X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf23cc5-2f34-447f-b417-220ba31a483d",
   "metadata": {},
   "source": [
    "#### Conclusion for the scikit-learn logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e26652-6810-4bb9-b565-93ce2996f0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nFINAL STATISTICS\")\n",
    "# Print accuracy metrics\n",
    "print(\"\\nACCURACY\")\n",
    "print_metrics(\"Accuracy\", accuracy_results, num_iterations)\n",
    "# Print precision metrics\n",
    "print(\"\\nPRECISION\")\n",
    "print_metrics(\"Precision\", precision_results, num_iterations)\n",
    "# Print recall metrics\n",
    "print(\"\\nRECALL\")\n",
    "print_metrics(\"Recall\", recall_results, num_iterations)\n",
    "# Print F1 score metrics\n",
    "print(\"\\nF1\")\n",
    "print_metrics(\"F1 Score\", f1_score_results, num_iterations)\n",
    "\n",
    "# Clean up remaining variables\n",
    "del precision_results, recall_results, f1_score_results, accuracy_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63411ad7-2a52-4049-a44a-9a4db117fdd6",
   "metadata": {},
   "source": [
    "## 3.2.2. Arbore de Decizie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7f57b5-ddc8-448e-b6bc-b0057e174a56",
   "metadata": {},
   "source": [
    "### 1. Implementare folosind scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993dca31-93c1-4ba0-a021-5ec0b8f1e4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_tree_scikit_learn(X, y, step_percent=100):\n",
    "    # Iterate over the scalers\n",
    "    for scaler_name, scaler in scalers.items():\n",
    "        # Split the dataset into training and testing sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "        \n",
    "        # Scalarea datelor de antrenare\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "    \n",
    "        # Create and train the decision tree model\n",
    "        model = DecisionTreeClassifier(random_state=None)\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "\n",
    "        # Make predictions on the test set\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        \n",
    "        # Evaluate the model\n",
    "        test_accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "        # Afișare și salvare metrici\n",
    "        print(f\"Acuratete folosind {scaler_name}: {test_accuracy}\")\n",
    "        print(f\"Precision folosind {scaler_name}: {precision}\")\n",
    "        print(f\"Recall folosind {scaler_name}: {recall}\")\n",
    "        print(f\"F1 Score folosind {scaler_name}: {f1}\")\n",
    "        \n",
    "        # Save metric values in dictionaries\n",
    "        if scaler_name not in precision_results:\n",
    "            precision_results[scaler_name] = []\n",
    "        precision_results[scaler_name].append(precision)\n",
    "        if scaler_name not in recall_results:\n",
    "            recall_results[scaler_name] = []\n",
    "        recall_results[scaler_name].append(recall)\n",
    "        if scaler_name not in f1_score_results:\n",
    "            f1_score_results[scaler_name] = []\n",
    "        f1_score_results[scaler_name].append(f1)\n",
    "        if scaler_name not in accuracy_results:\n",
    "            accuracy_results[scaler_name] = []\n",
    "        accuracy_results[scaler_name].append(test_accuracy)\n",
    "\n",
    "        # Clean up variables\n",
    "        del model\n",
    "\n",
    "    # Clean up remaining variables\n",
    "    del X_train, X_test, y_train, y_test, X_train_scaled, X_test_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51032cf5-4d66-433b-8bd3-31d1c6e77d7e",
   "metadata": {},
   "source": [
    "#### Rulam de 10 ori algoritmul pe sample-uri random ale datasetului initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83b9043-85c4-43dc-aea1-02b8f27671d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print information about the training dataset\n",
    "(N, D) = df.shape\n",
    "print(f\"Training dataset size: {N}\")\n",
    "print(f\"Number of features: {D - 1}\")\n",
    "\n",
    "# Create dictionaries to store metric values\n",
    "precision_results = {}\n",
    "recall_results = {}\n",
    "f1_score_results = {}\n",
    "accuracy_results = {}\n",
    "\n",
    "for iteration in range(num_iterations):\n",
    "    # Amestecarea (randomizarea) rândurilor\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    X, y = preprocess_data(df.copy())\n",
    "\n",
    "    print(f\"\\nRUN NUMBER {iteration + 1}\")\n",
    "    # Apelul funcției cu datele specifice\n",
    "    decision_tree_scikit_learn(X, y, step_percent=STEP_PERCENT)\n",
    "    del X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6276edd3-57d5-42ea-8d72-43d0a9600b50",
   "metadata": {},
   "source": [
    "#### Conclusion for the scikit-learn logistic regression¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffbb1e0-9c0a-431f-b591-9d73872bd851",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nFINAL STATISTICS\")\n",
    "# Print accuracy metrics\n",
    "print(\"\\nACCURACY\")\n",
    "print_metrics(\"Accuracy\", accuracy_results, num_iterations)\n",
    "# Print precision metrics\n",
    "print(\"\\nPRECISION\")\n",
    "print_metrics(\"Precision\", precision_results, num_iterations)\n",
    "# Print recall metrics\n",
    "print(\"\\nRECALL\")\n",
    "print_metrics(\"Recall\", recall_results, num_iterations)\n",
    "# Print F1 score metrics\n",
    "print(\"\\nF1\")\n",
    "print_metrics(\"F1 Score\", f1_score_results, num_iterations)\n",
    "\n",
    "# Clean up remaining variables\n",
    "del precision_results, recall_results, f1_score_results, accuracy_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97881e82-f007-4a23-bdde-2764465a8810",
   "metadata": {},
   "source": [
    "### 2. Implementare manuală"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92b40e8-fd6c-4e30-b719-d7dc7b1fb8b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "44dd5e07-34f6-4570-95e8-55e506c0c7d7",
   "metadata": {},
   "source": [
    "## Metricile de evaluare a modelului"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176b0346-a0e7-4294-818c-75c5825dc4b8",
   "metadata": {},
   "source": [
    "#### ACCURACY = (True Positives + True Negatives) / Total Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce0b796-9817-42ab-9a00-e9f97d72c821",
   "metadata": {},
   "source": [
    "Definitie: Acuratețea este o măsură a proporției de exemple clasificate corect de model din totalul de exemple.\n",
    "Este dată de raportul dintre numărul de exemple clasificate corect (True Positives + True Negatives) și totalul de exemple.\n",
    "Utilitate: Acuratețea este utilă în evaluarea generală a performanței modelului, \n",
    "dar poate fi înșelătoare în cazul seturilor de date cu dezechilibre între clase (imbalanced datasets).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad4d867-25e8-45c9-8ee6-b11d9e1efab8",
   "metadata": {},
   "source": [
    "#### PRECISION = True Positives / (True Positivies + False Positives)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78afcb27-0029-44f3-b6f1-593c58d11412",
   "metadata": {},
   "source": [
    "​Definiție: Precizia este măsura proporției de exemple pozitive identificate corect de model din totalul de exemple clasificate ca pozitive.\n",
    "Este dată de raportul dintre True Positives și suma dintre True Positives și False Positives.\n",
    "Utilitate: Precizia indică cât de \"precis\" este modelul atunci când clasifică exemplele pozitive.\n",
    "Este importantă în situațiile în care costurile pentru clasificarea greșită a unui exemplu pozitiv sunt ridicate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f5024e-9325-4331-9394-e93aa4ce2009",
   "metadata": {},
   "source": [
    "#### RECALL = True Positives / (True Positives + False Negatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a65eac3-b94c-4777-a7c3-bdc8647bc8c1",
   "metadata": {},
   "source": [
    "Definiție: Recuperarea (sau Sensitivity) este măsura proporției de exemple pozitive identificate corect de model din totalul de exemple pozitive.\n",
    "Este dată de raportul dintre True Positives și suma dintre True Positives și False Negatives.\n",
    "Utilitate: Recuperarea indică cât de bine modelul identifică toate exemplele pozitive. \n",
    "Este importantă atunci când nu doriți să ratați exemple pozitive și costurile pentru clasificarea greșită a unui exemplu negativ nu sunt prea mari."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8e1361-234d-4bd3-b113-24e9f3a99477",
   "metadata": {},
   "source": [
    "#### F1 = 2 x Precision x Recall / (Precision + Recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd748b6-63f1-45be-9030-0377b4d73e01",
   "metadata": {},
   "source": [
    "Definiție: Scorul F1 este media armonică dintre precizie și recuperare (recall). \n",
    "A fost conceput pentru a oferi o măsură echilibrată între cele două.\n",
    "Utilitate: Scorul F1 este util în situațiile în care vă interesează un echilibru între precizie și recuperare.\n",
    "Este deosebit de util în scenariile cu seturi de date dezechilibrate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1b60b6-adc2-4c0c-942a-422d1e29f59f",
   "metadata": {},
   "source": [
    "# Concluzii"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfb05ee-04c2-4df2-9206-f208b358b57a",
   "metadata": {},
   "source": [
    "Observam rezultate impartite intre cei 2 algoritmi (variantele implementate cu ajutorul bibliotecii scikit-learn). \n",
    "Consider ca metricile cele mai importante sunt Acuratetea predictiei si F1.\n",
    "Logistic Regression reuseste acuratetea predictiei putin mai mare,\n",
    "dar ca trade-off observam ca nu este la fel de stabil ca Decision Tree Clasifier, care obtine F1 mai bun cu 35%-40%."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
